{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from dateutil.parser import parse\n",
    "from SSURO_soil_data_functions import retrieve_soil_info_by_ID, get_table_headers, get_ssurgo_inventory, impute_data\n",
    "from fancyimpute import KNN\n",
    "from math import log\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaxtonRawls(pSand, pClay, pOM):\n",
    "    pSand = pSand/100\n",
    "    pClay = pClay/100\n",
    "    pOM = pOM/100\n",
    "    \n",
    "    # calc LL15 (theta_1500)\n",
    "    theta_1500t = -0.024*pSand + 0.487*pClay + 0.006*pOM + 0.005*pSand*pOM - 0.013*pClay*pOM + 0.068*pSand*pClay + 0.031\n",
    "    LL15 = theta_1500t + (0.14*theta_1500t - 0.02)\n",
    "    LL15 = round(max(0.01, min(0.99, LL15)),3)\n",
    "    \n",
    "    # calc DUL (theta_33)\n",
    "    theta_33t = -0.251*pSand + 0.195*pClay + 0.011*pOM +0.006*pSand*pOM - 0.027*pClay*pOM + 0.452*pSand*pClay + 0.299\n",
    "    DUL = theta_33t + (1.283*theta_33t**2 - 0.374*theta_33t - 0.015)\n",
    "    DUL = round(max(0.01, min(0.99, DUL)),3)\n",
    "    \n",
    "    # calc SAT-33 KPa moisture\n",
    "    theta_sat33t = 0.278*pSand + 0.034*pClay + 0.022*pOM - 0.018*pSand*pOM - 0.027*pClay*pOM - 0.584*pSand*pClay + 0.078\n",
    "    theta_sat33 = theta_sat33t + (0.636*theta_sat33t - 0.107)\n",
    "    \n",
    "    # calc SAT\n",
    "    SAT = DUL + theta_sat33 - 0.097*pSand + 0.043\n",
    "    SAT = round(max(0.01, min(0.99, SAT)),3)\n",
    "    \n",
    "    # calc BD\n",
    "    BD = (1 - SAT)*2.65\n",
    "    BD = round(max(1.0, min(2.1, BD)),3)\n",
    "  \n",
    "    # calc ksat (saturated water conductivity)\n",
    "    lambd = (log(DUL)-log(LL15)) / (log(1500)-log(33))\n",
    "    ksat = 1930*((SAT-DUL)**(3-lambd))\n",
    "    SWCON = round(0.15 + min(ksat,75)/100, 3)\n",
    "    \n",
    "    res = []\n",
    "    res = [BD, LL15*100, DUL*100, SAT*100, SWCON*100,ksat]\n",
    "    #names(res) = c(\"BD\", \"LL15\", \"DUL\", \"SAT\", \"SWCON\",\"KSAT\")\n",
    "  \n",
    "    return(res)\n",
    "    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.718408660039206"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SaxtonRawls(.32,40.5,.25)[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                        texdesc  \\\n",
      "0          13             Very gravelly clay   \n",
      "1          14  Very gravelly sandy clay loam   \n",
      "2          15       Very gravelly sandy clay   \n",
      "3          16        Very gravelly clay loam   \n",
      "4          17                      Silt loam   \n",
      "\n",
      "                                          taxclname  cec7  texture  \\\n",
      "0  Fine, mixed, active, thermic Udertic Paleustalfs  22.0    GRV-C   \n",
      "1  Fine, mixed, active, thermic Udertic Paleustalfs  22.0  GRV-SCL   \n",
      "2  Fine, mixed, active, thermic Udertic Paleustalfs  22.0   GRV-SC   \n",
      "3  Fine, mixed, active, thermic Udertic Paleustalfs  22.0   GRV-CL   \n",
      "4  Fine, mixed, active, thermic Udertic Paleustalfs  10.5      SIL   \n",
      "\n",
      "         ksat  claytotal  ptotal  hzthk  dbovendry  ...  \\\n",
      "0   28.255389       40.5     NaN   58.0       2.01  ...   \n",
      "1   28.255389       40.5     NaN   58.0       2.01  ...   \n",
      "2   28.255389       40.5     NaN   58.0       2.01  ...   \n",
      "3   28.255389       40.5     NaN   58.0       2.01  ...   \n",
      "4  160.161350       17.5     NaN   18.0       1.50  ...   \n",
      "\n",
      "   WaterTableNitrificationInhibitor  WaterTableDenitrificationInhibitor  \\\n",
      "0                                 0                                   0   \n",
      "1                                 0                                   0   \n",
      "2                                 0                                   0   \n",
      "3                                 0                                   0   \n",
      "4                                 0                                   0   \n",
      "\n",
      "   NO3Exco  NO3FIP  NH4Exco  NH4FIP  UreaExco UreaFIP  ClExco  ClFIP  \n",
      "0        0       1      100       1         0       1       0      1  \n",
      "1        0       1      100       1         0       1       0      1  \n",
      "2        0       1      100       1         0       1       0      1  \n",
      "3        0       1      100       1         0       1       0      1  \n",
      "4        0       1      100       1         0       1       0      1  \n",
      "\n",
      "[5 rows x 94 columns]\n"
     ]
    }
   ],
   "source": [
    "###Definitions\n",
    "df = pd.read_csv(\"/Users/timreeves/Desktop/Python_Stuff/maize_yield_pred/data/Soil_data/OKLAHOMA@JOHNSTON.csv\")\n",
    "def Trunk(row, column_name, firsttrunk, secondtrunk = None):\n",
    "    if row[column_name] < firsttrunk:\n",
    "        return firsttrunk\n",
    "    elif row[column_name] > secondtrunk:\n",
    "        return secondtrunk\n",
    "    else:\n",
    "        return row[column_name]\n",
    "def Soil_Physical_Prop():\n",
    "    df[\"bd\"] = df.apply(lambda row: Trunk(row, \"dbthirdba\", 0.9, 1.8),axis=1)\n",
    "    df[\"ksat\"] = df.apply(lambda row: \n",
    "                          (min(row[\"ksat\"]*100/1.157, \n",
    "                               SaxtonRawls(row[\"sandtotal\"] , row[\"claytotal\"], row[\"om\"])[5]*24)), axis=1)\n",
    "    df[\"ksat\"] = df.apply(lambda row: Trunk(row, \"ksat\", 0, 500), axis=1)\n",
    "    df[\"sat\"] = df.apply(lambda row: SaxtonRawls(row[\"sandtotal\"] , row[\"claytotal\"], row[\"om\"])[3]/100, axis=1)\n",
    "    df[\"PO\"] = df.apply(lambda row: 1-row[\"bd\"]/2.65, axis=1)\n",
    "    df[\"Salb\"] = 0.15\n",
    "    df[\"MWCON\"] = 1\n",
    "    df[\"dul\"] = df.apply(lambda row: row[\"wthirdba\"]/100, axis=1)\n",
    "    df[\"dul\"].fillna(0, inplace=True)\n",
    "    df[\"ll\"] = df.apply(lambda row: row[\"wfifteenba\"]/100, axis=1)\n",
    "    df[\"ll\"].fillna(0, inplace=True)\n",
    "    df[\"SWCON\"] = df.apply(lambda row: (row[\"PO\"]-row[\"dul\"]/row[\"PO\"]), axis=1)\n",
    "    df[\"center\"] = df.apply(lambda row: row[\"hzdept\"]+(row[\"hzthk\"]/2), axis=1)\n",
    "    df[\"AirDry\"] = df.apply(lambda row: 0.9*row[\"ll\"] if row[\"center\"]<=15 else 0.95*row[\"ll\"] if \n",
    "                            row[\"center\"]<=30 else 1*row[\"ll\"], axis=1)\n",
    "    df[\"U\"] = df.apply(lambda row: (5+.175*row[\"claytotal\"]) if row[\"claytotal\"]<=20\n",
    "                       else (7.5 + 0.05*row[\"claytotal\"]) if row[\"claytotal\"] <=40 \n",
    "                       else (11.5-.05*row[\"claytotal\"])if row[\"claytotal\"]<=50 \n",
    "                       else (12.75-0.075*row[\"claytotal\"]) if row[\"claytotal\"]<=70\n",
    "                       else (11-0.05*row[\"claytotal\"]) if row[\"claytotal\"]<=80\n",
    "                       else 0, axis=1)\n",
    "    df[\"cana\"] = df.apply(lambda row: (0.025*row[\"claytotal\"]+3.25) if row[\"claytotal\"]<=30 \n",
    "                  else (4) if row[\"claytotal\"]<=50 \n",
    "                  else (-0.025*row[\"claytotal\"]+5.25) if row[\"claytotal\"]<=70 \n",
    "                  else (3.5) if row[\"claytotal\"]<=80 \n",
    "                  else 0, axis=1)\n",
    "    \n",
    "                       \n",
    "    \n",
    "    \n",
    "    df[\"DiffusConst\"] = 40\n",
    "    df[\"DiffusSlope\"] = 16\n",
    "    df[\"CN2\"] = 80\n",
    "    df[\"CNRed\"] = 20\n",
    "    df[\"CNCov\"] = 0.8\n",
    "    df[\"EnrAcoeff\"] = 7.4\n",
    "    df[\"EnrBcoeff\"] = 0.2\n",
    "    df[\"XF_maize\"] = 1\n",
    "    df[\"KL_maize\"] = df.apply(lambda row: .08 if row[\"center\"]<=20 else (.09 * math.exp(-.007*row[\"center\"])), axis=1)\n",
    "    df[\"e\"] = 0.5\n",
    "    df[\"ph\"] = df.apply(lambda row: 0.52+1.06*row[\"ph1to1h2o\"], axis=1)\n",
    "    df[\"OC\"] = df.apply(lambda row: row[\"om\"]/1.72, axis=1)\n",
    "    ###horizon$OC <- c(horizon$OC[1],\n",
    "                    #ifelse(horizon$center[-1] >= 100 & diff(horizon$OC) == 0,\n",
    "                           #horizon$OC[1]*exp(horizon$center[-1]*-0.035),\n",
    "                           #horizon$OC))\n",
    "                #This new addition to OC doesn't make much sense to me\n",
    "                #Indenxing to calculate all rows based off of the first or last?\n",
    "    df[\"FInert\"] = df.apply(lambda row: 0.4 if row[\"center\"]<=1\n",
    "                           else 0.4 if row[\"center\"]<=10\n",
    "                           else .008*row[\"center\"]+.32 if row[\"center\"]<60\n",
    "                           else .8 if row[\"center\"]<=120\n",
    "                           else .0032*row[\"center\"]+.42 if row[\"center\"]<180\n",
    "                           else .99 if row[\"center\"]<=300\n",
    "                           else 0, axis=1)\n",
    "    df[\"FBiom\"] = df.apply(lambda row: 0.4 if row[\"center\"]<=10\n",
    "                           else .055-.0015*row[\"center\"] if row[\"center\"]<=20\n",
    "                           else .03-.0005*row[\"center\"] if row[\"center\"]<=30\n",
    "                           else .0216-.0002*row[\"center\"] if row[\"center\"]<60\n",
    "                           else .01 if row[\"center\"]<300\n",
    "                           else 0, axis=1)\n",
    "    \n",
    "    \n",
    "    df[\"RootCN\"] = 45\n",
    "    df[\"SoilCN\"] = 13\n",
    "    df[\"RootWt\"] = 1000\n",
    "    df[\"KDul\"] = 0.1\n",
    "    df[\"PSIDul\"] = -300\n",
    "    df[\"VC\"] = True\n",
    "    df[\"DTmin\"] = 0\n",
    "    df[\"DTmax\"] = 1440\n",
    "    df[\"MaxWaterIncrement\"] = 10\n",
    "    df[\"SpaceWeightingFactor\"] = 0\n",
    "    df[\"SoluteSpaceWeightingFactor\"] = 0\n",
    "    df[\"Diagnostics\"] = False\n",
    "    df[\"Dis\"] = 15\n",
    "    df[\"Disp\"] = 1\n",
    "    df[\"A\"] = 1\n",
    "    df[\"DTHC\"] = 0\n",
    "    df[\"DTHP\"] = 1\n",
    "    df[\"WaterTableCl\"] = 0\n",
    "    df[\"WaterTableNO3\"] = 0\n",
    "    df[\"WaterTableNH4\"] = 0\n",
    "    df[\"WaterTableUrea\"] = 0\n",
    "    df[\"WaterTableTracer\"] = 0\n",
    "    df[\"WaterTableMineralisationInhibitor\"] = 0\n",
    "    df[\"WaterTableUreaseInhibitor\"] = 0\n",
    "    df[\"WaterTableNitrificationInhibitor\"] = 0\n",
    "    df[\"WaterTableDenitrificationInhibitor\"] = 0\n",
    "    #swim Thickness$double  <- 1000\n",
    "    df[\"NO3Exco\"] = 0\n",
    "    df[\"NO3FIP\"] =  1\n",
    "    df[\"NH4Exco\"] = 100\n",
    "    df[\"NH4FIP\"] = 1\n",
    "    df[\"UreaExco\"] = 0\n",
    "    df[\"UreaFIP\"] = 1\n",
    "    df[\"ClExco\"] = 0\n",
    "    df[\"ClFIP\"] = 1\n",
    "    #df[\"DrainDepth\"] = drainage_parms$DrainDepth\n",
    "    #horizon$DrainSpacing  <- drainage_parms$DrainSpacing\n",
    "    #horizon$DrainRadius  <- drainage_parms$DrainRadius\n",
    "    #horizon$Klat  <- drainage_parms$Klat\n",
    "    #df[\"ImpermDepth\"] = max(df[\"bottom\"])*10 + 500 \n",
    "    #df[\"WaterTableDepth\"]  <- df[\"watertable_g\"]*10\n",
    "    df.to_csv(\"/Users/timreeves/Desktop/test.csv\")\n",
    "    print(df.head())\n",
    "Soil_Physical_Prop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.01000\n",
       "1     0.01000\n",
       "2     0.01000\n",
       "3     0.01000\n",
       "4     0.40000\n",
       "5     0.01000\n",
       "6     0.01000\n",
       "7     0.01000\n",
       "8     0.01000\n",
       "9     0.01000\n",
       "10    0.01000\n",
       "11    0.02800\n",
       "12    0.01000\n",
       "13    0.01000\n",
       "14    0.01000\n",
       "15    0.01000\n",
       "16    0.01000\n",
       "17    0.01000\n",
       "18    0.01000\n",
       "19    0.01000\n",
       "20    0.01000\n",
       "21    0.01340\n",
       "22    0.01340\n",
       "23    0.01340\n",
       "24    0.40000\n",
       "25    0.01350\n",
       "26    0.01350\n",
       "27    0.01000\n",
       "28    0.01000\n",
       "29    0.01000\n",
       "       ...   \n",
       "54    0.01450\n",
       "55    0.01000\n",
       "56    0.03250\n",
       "57    0.01000\n",
       "58    0.01000\n",
       "59    0.01000\n",
       "60    0.01000\n",
       "61    0.01000\n",
       "62    0.01040\n",
       "63    0.01040\n",
       "64    0.02800\n",
       "65    0.01000\n",
       "66    0.01000\n",
       "67    0.01200\n",
       "68    0.01000\n",
       "69    0.01000\n",
       "70    0.01000\n",
       "71    0.40000\n",
       "72    0.01000\n",
       "73    0.01000\n",
       "74    0.40000\n",
       "75    0.01000\n",
       "76    0.01000\n",
       "77    0.01000\n",
       "78    0.01000\n",
       "79    0.01000\n",
       "80    0.01000\n",
       "81    0.01725\n",
       "82    0.01725\n",
       "83    0.01725\n",
       "Name: FBiom, Length: 84, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"FBiom\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_dir=\"../data/Soil_data/\"\n",
    "os.system(\"mkdir \"+soil_dir+\"tmp_soil/\")\n",
    "#load county coordinate data from working phenotype table (produced in script 1)\n",
    "soil_areas = pd.read_csv(\"../data/Phenotype_data/work_set_yields_by_county.csv\", index_col=0)\n",
    "soil_areas.drop_duplicates(subset=\"ID\", inplace=True)\n",
    "soil_areas = soil_areas[[\"ID\",\"County\",\"State\",\"Latitude\",\"Longitude\"]]\n",
    "soil_areas.dropna(inplace=True)\n",
    "#place random county from each state at the top of the list so that one county per state is run first as a test\n",
    "soil_areas = soil_areas.sample(frac=1, random_state=54321)\n",
    "soil_areas[\"tmp\"] = soil_areas.duplicated(\"State\")\n",
    "soil_areas = soil_areas.sort_values(\"tmp\").reset_index(drop=True)\n",
    "#for the moment we only care abotu downloading entire countys (in most cases the same thing as a single \n",
    "#SSURGO soil survey area). For that reason we will use an area of 0.001 degreas in all directions around\n",
    "#the provided county center data. This is just an area which we are confident lies within the county and\n",
    "#is used to pull down the data for the area of the entire county (assumed here to be the same as the\n",
    "#soil survey area)\n",
    "degs_to_add=0.001\n",
    "soil_areas[\"North\"] = soil_areas[\"Latitude\"]+degs_to_add\n",
    "soil_areas[\"South\"] = soil_areas[\"Latitude\"]-degs_to_add\n",
    "soil_areas[\"East\"] = soil_areas[\"Longitude\"]+degs_to_add\n",
    "soil_areas[\"West\"] = soil_areas[\"Longitude\"]-degs_to_add\n",
    "\n",
    "#CSV Patrick made containing names of important soil categories\n",
    "#TabRead = pd.read_csv(\"../data/General/TableColumnDescriptionsReport.csv\", encoding=\"ISO-8859-1\", skiprows=1)\n",
    "TabRead = pd.read_csv(\"../data/General/Revised Table Selections Soil Table.csv\", encoding=\"ISO-8859-1\", skiprows=1)\n",
    "#remove tables that we don't care about\n",
    "TabRead = TabRead[TabRead[\"Usable\"]==\"yes\"]\n",
    "desired_tables = TabRead[\"Table Physical Name\"].unique().tolist()\n",
    "#import SSURGO table connections\n",
    "table_relationships = pd.read_csv(\"../data/General/SSURGO_table_relationships.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 2613 UTAH@JUAB\n",
      "Querying SSURGO to determine which survey area(s) is/are needed.\n",
      "https://sdmdataaccess.nrcs.usda.gov/Spatial/SDMNAD83Geographic.wfs?Service=WFS&Version=1.0.0&Request=GetFeature&Typename=SurveyAreaPoly&BBOX=-112.79526,39.70982,-112.79326,39.71182\n",
      "Downloading file: http://websoilsurvey.sc.egov.usda.gov/DSD/Download/Cache/SSA/wss_SSA_UT617_[2019-09-16].zip\n",
      " to: ../data/Soil_data/tmp_soil/UT617_[2019-09-16].zip\n",
      "15 mapunit mukey component (95, 24)\n",
      "20 component cokey chorizon (238, 132)\n",
      "48 chorizon chkey chtexturegrp (608, 302)\n",
      "2 / 2613 NORTH_DAKOTA@MORTON\n",
      "Querying SSURGO to determine which survey area(s) is/are needed.\n",
      "https://sdmdataaccess.nrcs.usda.gov/Spatial/SDMNAD83Geographic.wfs?Service=WFS&Version=1.0.0&Request=GetFeature&Typename=SurveyAreaPoly&BBOX=-101.28074,46.71278,-101.27874,46.71478\n",
      "Downloading file: http://websoilsurvey.sc.egov.usda.gov/DSD/Download/Cache/SSA/wss_SSA_ND059_[2019-09-16].zip\n",
      " to: ../data/Soil_data/tmp_soil/ND059_[2019-09-16].zip\n",
      "15 mapunit mukey component (147, 24)\n",
      "20 component cokey chorizon (887, 132)\n",
      "48 chorizon chkey chtexturegrp (3855, 302)\n",
      "3 / 2613 OKLAHOMA@JOHNSTON\n",
      "Querying SSURGO to determine which survey area(s) is/are needed.\n",
      "https://sdmdataaccess.nrcs.usda.gov/Spatial/SDMNAD83Geographic.wfs?Service=WFS&Version=1.0.0&Request=GetFeature&Typename=SurveyAreaPoly&BBOX=-96.65525,34.31245,-96.65325,34.31445\n",
      "Downloading file: http://websoilsurvey.sc.egov.usda.gov/DSD/Download/Cache/SSA/wss_SSA_OK069_[2019-09-16].zip\n",
      " to: ../data/Soil_data/tmp_soil/OK069_[2019-09-16].zip\n",
      "15 mapunit mukey component (51, 24)\n",
      "20 component cokey chorizon (153, 132)\n",
      "48 chorizon chkey chtexturegrp (493, 302)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-925864696f1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"failed.  Added to list for future attempts.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mfailed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mall_processed_soils\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_processed_soils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    226\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                        copy=copy, sort=sort)\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No objects to concatenate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "failed = {}\n",
    "all_processed_soils=[]\n",
    "tablesHeaders = get_table_headers()\n",
    "count=0\n",
    "for ID in soil_areas[\"ID\"].unique()[:3]:\n",
    "    count+=1\n",
    "    print(count,\"/\",len(soil_areas[\"ID\"].unique()), ID)\n",
    "    #Get SSURGO inventory data for desired area of interest\n",
    "    try:\n",
    "        MergedCsv, spatial, tabular, majcomp = retrieve_soil_info_by_ID(ID, soil_dir, soil_areas, table_relationships,\n",
    "                                                               TabRead, desired_tables, verbose=True)\n",
    "    except Exception as e:\n",
    "        print(ID, \"failed.  Added to list for future attempts.\")\n",
    "        failed[ID] = str(e)\n",
    "all_processed_soils = pd.concat(all_processed_soils, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
